{
 "metadata": {
  "name": "",
  "signature": "sha256:0179832c81a2c142992dfdad5867a5ffcf15ea8073a4dc5d00698be11b45ef7c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# compute the Jaccard distance between two sentences\n",
      "def DistJaccard(str1, str2):\n",
      "    if str1 != '' and str2 != '':\n",
      "        str1 = set(str1.split())\n",
      "        str2 = set(str2.split())\n",
      "        return 1.0 - float(len(str1 & str2)) / len(str1 | str2)\n",
      "    else:\n",
      "        return numpy.nan"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# functions to normaliza data (remove stopwords, ponctuation and numbers)\n",
      "with open('stopwords_fr.txt') as f:\n",
      "    Stopwords = [r.rstrip() for r in f.readlines()]\n",
      "\n",
      "exclude = set(string.punctuation)\n",
      "def remove_ponctuation(s):\n",
      "    return ''.join(ch for ch in s if ch not in exclude)\n",
      "\n",
      "def remove_stopwords(s):\n",
      "    return ' '.join(word for word in s.split() if word not in set(Stopwords))\n",
      "\n",
      "def normalize_data(s):\n",
      "    return remove_stopwords(remove_ponctuation(str(s))).upper()\n",
      "\n",
      "# keep only strings\n",
      "def toString(sentence):\n",
      "    out = ''\n",
      "    if str(sentence) != 'nan':\n",
      "        for word in sentence.split():\n",
      "            if isinstance(word, basestring):\n",
      "                out += (\" \" + word)\n",
      "    return out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# define tf-idf functions\n",
      "def tf(word, blob):\n",
      "    return blob.words.count(word) / len(blob.words)\n",
      "\n",
      "def n_containing(word, bloblist):\n",
      "    return sum(1 for blob in bloblist if word in blob)\n",
      "\n",
      "def idf(word, bloblist):\n",
      "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
      "\n",
      "def tfidf(word, blob, bloblist):\n",
      "    return tf(word, blob) * idf(word, bloblist)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute tf-idf for ingredient field\n",
      "\n",
      "# Ingredients corpus\n",
      "bloblist_ing = [tb(normalize_data(toString(ing))) for ing in (Products_GS['ingredients'].tolist() + Products_GS_tc['ingredients'].tolist())]\n",
      "\n",
      "# Compute TfIdf for ingredients, select the 10 most important words\n",
      "words_tfidf = []\n",
      "for i, blob in enumerate(bloblist_ing):\n",
      "    #print(\"Top words in document {}\".format(i + 1))\n",
      "    scores = {word: tfidf((word), blob, bloblist_ing) for word in blob.words}\n",
      "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
      "    words = ''\n",
      "    for word, score in sorted_words[:10]:\n",
      "        #print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
      "        words += (' ' + word)\n",
      "    words_tfidf.append(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}